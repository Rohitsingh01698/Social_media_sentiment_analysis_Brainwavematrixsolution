{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOvipvr76DvtI25xkmJ/VhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohitsingh01698/Social_media_sentiment_analysis_Brainwavematrixsolution/blob/main/Twitter_sentiment_analysis_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analyze this Twitter dataset to understand public sentiment towards specific topics, products, or events. Use natural language Analyze this Twitter CSV dataset using social media sentiment analysis processing (NLP) techniques to preprocess text data, extract sentiment scores, and visualize trends over time."
      ],
      "metadata": {
        "id": "8kbY-3Hasjqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "8rTGw7A1sosa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data=pd.read_csv(r\"/content/twitter.csv\")\n",
        "twitter_data.head()\n"
      ],
      "metadata": {
        "id": "xiBqZ7bts7uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data.info()"
      ],
      "metadata": {
        "id": "OfNibhbEtpsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data.describe()"
      ],
      "metadata": {
        "id": "zNNIRoY9ts59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Handle Missing Values\n",
        "twitter_data.isnull().sum()\n",
        "twitter_data.dropna(subset=['tweet'], inplace=True)\n"
      ],
      "metadata": {
        "id": "_UonZaoR0yaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Remove Irrelevant Characters\n",
        "def clean_tweet(tweet):\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)\n",
        "    tweet = re.sub(r'@\\S+', '', tweet)\n",
        "    tweet = re.sub(r'#\\S+', '', tweet)\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
        "    return tweet\n",
        "\n",
        "twitter_data['tweet'] = twitter_data['tweet'].apply(clean_tweet)\n"
      ],
      "metadata": {
        "id": "9z_gS6oGz58H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Duplicates\n",
        "twitter_data.drop_duplicates(subset=['tweet'], inplace=True)\n"
      ],
      "metadata": {
        "id": "_wm7r1sX1MJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase Conversion\n",
        "twitter_data['tweet'] = twitter_data['tweet'].str.lower()\n"
      ],
      "metadata": {
        "id": "hhWy1YPq1TJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_data.head()"
      ],
      "metadata": {
        "id": "v6wRjtyQz9IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Shape and Information\n",
        "print(\"Data Shape:\", twitter_data.shape)\n",
        "display(twitter_data.info())\n",
        "\n",
        "# Sentiment Distribution\n",
        "sentiment_counts = twitter_data['label'].value_counts()\n",
        "print(\"\\nSentiment Distribution:\\n\", sentiment_counts)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sentiment_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Text Length Analysis\n",
        "twitter_data['tweet_length'] = twitter_data['tweet'].str.len()\n",
        "tweet_length_stats =twitter_data['tweet_length'].describe()\n",
        "print(\"\\nTweet Length Statistics:\\n\", tweet_length_stats)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(twitter_data['tweet_length'], bins=30, color='lightgreen', edgecolor='black')\n",
        "plt.title('Distribution of Tweet Lengths')\n",
        "plt.xlabel('Tweet Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "A3nAYt5JvSeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt_tab' data\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    # Lowercasing\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(tweet)\n",
        "\n",
        "    # Stop Word Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "\n",
        "twitter_data['processed_tweets'] = twitter_data['tweet'].apply(preprocess_tweet)\n",
        "display(twitter_data.head())"
      ],
      "metadata": {
        "id": "auke-xUCurRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "dJ2gsZrDyC1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    # Lowercasing\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(tweet)\n",
        "\n",
        "    # Stop Word Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "\n",
        "twitter_data['processed_tweets'] = twitter_data['tweet'].apply(preprocess_tweet)\n",
        "display(twitter_data.head())"
      ],
      "metadata": {
        "id": "FoJ1jTn52W1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Sentiment Analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "twitter_data['sentiment_score'] = twitter_data['processed_tweets'].apply(lambda tweet: analyzer.polarity_scores(tweet)['compound'])\n"
      ],
      "metadata": {
        "id": "-Acdf3tK20rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sentiment Analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "twitter_data['sentiment_score'] = twitter_data['processed_tweets'].apply(lambda tweet: analyzer.polarity_scores(tweet)['compound'])\n",
        "\n",
        "# 2. Extract Features\n",
        "def extract_hashtags(tweet):\n",
        "    return re.findall(r'#\\w+', tweet)\n",
        "\n",
        "def extract_mentions(tweet):\n",
        "    return re.findall(r'@\\w+', tweet)\n",
        "\n",
        "twitter_data['hashtags'] = twitter_data['tweet'].apply(extract_hashtags)\n",
        "twitter_data['mentions'] =twitter_data['tweet'].apply(extract_mentions)\n",
        "\n",
        "# 3. Explore Features\n",
        "print(twitter_data[['sentiment_score', 'tweet_length']].describe())\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(twitter_data['sentiment_score'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Sentiment Scores')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(twitter_data['tweet_length'], bins=20, color='lightgreen', edgecolor='black')\n",
        "plt.title('Distribution of Tweet Lengths')\n",
        "plt.xlabel('Tweet Length')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(twitter_data['tweet_length'], twitter_data['sentiment_score'], alpha=0.5, color='purple')\n",
        "plt.title('Sentiment Score vs. Tweet Length')\n",
        "plt.xlabel('Tweet Length')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.show()\n",
        "\n",
        "display(twitter_data.head())"
      ],
      "metadata": {
        "id": "nW5LgahiFuWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to convert 'id' to datetime\n",
        "try:\n",
        "    twitter_data['id'] = pd.to_datetime(twitter_data['id'])\n",
        "    time_series_possible = True\n",
        "    # Group by time and calculate rolling average sentiment\n",
        "    twitter_data['rolling_sentiment'] = twitter_data.groupby('label')['sentiment_score'].rolling(window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
        "except (ValueError, TypeError):\n",
        "    time_series_possible = False\n",
        "    print(\"The 'id' column cannot be converted to datetime objects. Time-series analysis is not possible.\")\n"
      ],
      "metadata": {
        "id": "mD2slxxzIKL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data by 'label' and calculate average sentiment\n",
        "if 'label' in twitter_data.columns:\n",
        "    sentiment_by_label = twitter_data.groupby('label')['sentiment_score'].mean().reset_index()\n",
        "    display(sentiment_by_label)\n",
        "else:\n",
        "    print(\"No 'label' column found in the dataframe.\")\n"
      ],
      "metadata": {
        "id": "eb4gUB7WINXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Line chart of rolling sentiment over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "for label in twitter_data['label'].unique():\n",
        "    subset = twitter_data[twitter_data['label'] == label]\n",
        "    plt.plot(subset['id'], subset['rolling_sentiment'], label=f'Label {label}')\n",
        "plt.title('Rolling Sentiment Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Rolling Sentiment')\n",
        "plt.legend()\n",
        "plt.savefig('sentiment_over_time.png')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "n1FuGdv0GIJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Bar chart of average sentiment by label\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(sentiment_by_label['label'], sentiment_by_label['sentiment_score'], color=['skyblue', 'salmon'])\n",
        "plt.title('Average Sentiment by Label')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Average Sentiment Score')\n",
        "plt.savefig('sentiment_by_label.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cs75Y-HzH4Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming 'label' column represents sentiment (0: Negative, 1: Positive)\n",
        "positive_text = \" \".join(twitter_data[twitter_data[\"label\"] == 1][\"tweet\"])\n",
        "# Filter using 'label' column, assuming 1 represents positive sentiment\n",
        "\n",
        "# Now you can use WordCloud since it's imported\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(positive_text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud: Positive Tweets\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C70ktEKrZgRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot sentiment distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "# Assuming 'label' column represents sentiment (0: Negative, 1: Positive)\n",
        "# Replace 'sentiment' with 'label' if it's the sentiment column\n",
        "twitter_data[\"label\"].value_counts().plot(kind=\"bar\", color=[\"skyblue\", \"salmon\", \"lightgreen\"])\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.grid(axis=\"y\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jaZpftbneouT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBroBUfMepov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Analysis Key Findings\n",
        "\n",
        "Sentiment Imbalance: The dataset exhibits a significant imbalance in sentiment labels, with label 0 (likely negative) being substantially more frequent than label 1 (likely positive). Further investigation is needed to understand the implications of this imbalance.\n",
        "\n",
        "Average Sentiment per Label: Label 0 has an average sentiment score of 0.089734, while Label 1 has -0.083558. The meaning of each label requires more context to determine the nature of the sentiment.\n",
        "\n",
        "Rolling Sentiment Trends: A 7-day rolling average of sentiment scores was calculated to identify trends over time, allowing for the visualization of sentiment fluctuations.\n",
        "\n",
        "Insights:\n",
        "\n",
        "Investigate data source and potential biases: Analyze the 'id' column and any available metadata to understand the time frame covered, tweet sources, and potential biases. This is crucial for interpreting the sentiment analysis accurately.\n",
        "Deep Dive into the meaning of the labels: Understand what the labels (0 and 1) represent in the context of the dataset, and look for any potential label inconsistencies.\n",
        "The processed and cleaned dataset gives a clear overview of public sentiment.\n",
        "\n",
        "This analysis can guide businesses, marketers, or researchers in understanding audience attitudes and making data-driven decisions."
      ],
      "metadata": {
        "id": "d4lJMy6MpGmK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7v85LqJrpT1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}